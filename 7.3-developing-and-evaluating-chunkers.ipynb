{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开发和评估分块器\n",
    "\n",
    "## 读取 IOB 格式与 CoNLL2000 分块语料库\n",
    "\n",
    "使用转换函数 [chunk.conllstr2tree()](https://www.nltk.org/_modules/nltk/chunk/util.html#conllstr2tree) 可以将 IOB 格式的字符串转换成树表示。此外，它允许我们选择使用语料库提供的三种块类型：NP、VP 和 PP 的任何子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"\"\"\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "\"\"\"\n",
    "\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ch07-tree-2.png](resources/ch07-tree-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK 的 corpus 模块包含了大量已分块的文本。CoNLL2000 语料库包含 27 万词的《华尔街日报》文本，分为“训练”和“测试”两部分，标注有词性标记和 IOB 格式分块标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print(conll2000.chunked_sents('train.txt')[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你看到的，CoNLL2000 语料库包含了三种块类型：NP 块如 a cup；VP 块如 told；PP 块如 of。由于现在我们唯一感兴趣的是 NP 块，我们可以使用 chunk_types 参数选择它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "test_sent = conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]\n",
    "print(type(test_sent))\n",
    "print(test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单评估和基准\n",
    "\n",
    "首先，我们为不创建任何块的分块器建立一个基准（baseline）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser('')\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.parse(test_sent))\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IOB 标记准确性表明超过三分之一的词被标注为 O，即没有出现在 NP 块中。然而，由于我们的标注器没有找到任何块，其精度、召回率和 F 度量均为零。现在让我们尝试一个初级的正则表达式分类器，查找以名词短语标记的特征字母（如 CD、DT 和 JJ）开头的标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP (NP a/DT cup/NN))\n",
      "  of/IN\n",
      "  (NP (NP coffee/NN))\n",
      "  ,/,\n",
      "  (NP (NP Mr./NNP Stone/NNP))\n",
      "  told/VBD\n",
      "  (NP (NP his/PRP$ story/NN))\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r'NP: {<[CDJNP].*>+}'\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(test_sent))\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法达到了不错的结果，但是我们可以采用更多数据驱动的方法改善它。这里我们定义了 UnigramChunker 类，使用 unigram 标注器给句子加块标记。这个类实现了 [nltk.ChunkParserI](https://www.nltk.org/_modules/nltk/chunk/api.html#ChunkParserI) 接口，定义了两个方法：一个构造函数，当我们建立新的 UnigramChunker 时调用；一个 parse 方法，用来给新句子分块。\n",
    "\n",
    "构造函数需要训练句子的一个链表，每个句子都是块树的形式。它首先通过 tree2conlltags 方法将块树转换成 IOB 标记，然后训练一个基于词性标记的 unigram 块标注器。\n",
    "\n",
    "parse 方法取一个已标注的句子作为输入，首先提取词性标记，然后使用在构造函数中训练过的标注器为词性标记标注 IOB 标记。接下来将块标记与原句组合，产生 conlltags。最后使用 conlltags2tree 将结果转换成一个块树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
    "                         for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (_, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) \n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个分块器相当不错，达到整体 F 度量 83% 的得分。现在我们来分析一下 unigram 标注器给每个词性标记分配了什么样的块标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
     ]
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                    for (_, pos) in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现大多数标点符号都出现在 NP 块外，除了货币符号 # 和 \\$ ；限定词（DT）和所有格（PRP\\$ 和 WP\\$）出现在 NP 块的开头，而名词类型（NN，NNP，NNPS，NNS）大多出现在 NP 块内。\n",
    "\n",
    "我们对 unigram 分块器稍作修改，建立一个 bigram 分块器，性能略有提升："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
    "                         for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (_, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) \n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练基于分类器的分块器\n",
    "\n",
    "无论是基于正则表达式的分块器还是 n-gram 分块器，决定创建什么块完全基于词性标记。然而有时词性标记不足以确定一个句子应如何分块。例如：\n",
    "\n",
    "a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
    "\n",
    "b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
    "\n",
    "这两句话的词性标记相同，但分块方式不同。第一句中 the farmer 和 rice 都是单独的块，而第二个句子中相应的部分 the computer monitor 是一个单独的块。因此，为了最大限度地提升分块的性能，我们需要使用词的内容信息作为词性标注的补充。\n",
    "\n",
    "我们包含词的内容信息的方法之一是使用基于**分类器**的标注器对句子分块。在下面的例子中包括两个类：第一个类与 6.1 节中的 ConsecutivePosTagger 类似，仅有的区别在于使用 MaxentClassifier 代替 NaiveBayesClassifier；第二个类是标注器类的一个包装器，将它变成一个分块器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train(train_set, trace=0)\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w, t), c) for (w, t, c) in\n",
    "                        nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w, t, c) for ((w, t), c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还需要定义用到的特征提取器 npchunk_features。首先，我们定义一个简单的特征提取器，它只提供当前标识符的词性标记。利用这个体征提取器的分块器性能与 unigram 分块器非常类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {'pos': pos}\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们再添加一个特征：前面词的词性标记。添加此特征允许分类器模拟相邻标记间的相互作用，由此产生的分块器与 bigram 分块器非常接近："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.6%%\n",
      "    Precision:     82.0%%\n",
      "    Recall:        87.2%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    return {'pos': pos, 'prevpos': prevpos}\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步，我们尝试把当前词增加为特征，可以发现这个特征确实提高了分块器的性能，大约 1.5 个百分点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.6%%\n",
      "    Precision:     84.6%%\n",
      "    Recall:        89.8%%\n",
      "    F-Measure:     87.1%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    return {'pos': pos, 'word': word, 'prevpos': prevpos}\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们尝试用多种附加特征扩展特征提取器，例如：预取特征、配对功能和复杂的语境特征等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.0%%\n",
      "    Precision:     88.3%%\n",
      "    Recall:        91.1%%\n",
      "    F-Measure:     89.7%%\n"
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = '<START>', '<START>'\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i - 1]\n",
    "    if i == len(sentence) - 1:\n",
    "        nextword, nextpos = '<END', 'END>'\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i + 1]\n",
    "    return {'pos': pos,\n",
    "            'word': word, \n",
    "            'prevpos': prevpos,\n",
    "            'nextpos': nextpos,\n",
    "            'prevpos+pos': '%s+%s' % (prevpos, pos),\n",
    "            'pos+nextpos': '%s+%s' % (pos, nextpos),\n",
    "            'tags-since-dt': tags_since_dt(sentence, i)}\n",
    "\n",
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))\n",
    "\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

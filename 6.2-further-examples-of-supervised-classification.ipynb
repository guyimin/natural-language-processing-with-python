{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有监督分类的更多例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子分割\n",
    "句子分割可以看作是一个标点符号的分类任务：每当我们遇到一个可能会结束一个句子的符号，如句号或问号，我们必须决定它是否终止了当前句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'START', 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will']\n",
      "[1, 20, 36, 38, 64, 66, 102, 134, 163, 199]\n",
      "101797\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []  # Store all words in a list.\n",
    "boundaries = set()  # Store all sentence end positions from tokens list above.\n",
    "offset = 0\n",
    "for sent in nltk.corpus.treebank_raw.sents():\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset - 1)\n",
    "    \n",
    "print(tokens[:10])\n",
    "print(sorted(list(boundaries))[:10])\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们为句子中的每个标点符号提供如下数据特征：\n",
    "1. next-word-capitalized：下一个单词是否首字母大写\n",
    "2. prevword：上一个单词的小写形式\n",
    "3. punct：当前标点符号\n",
    "4. prev-word-is-one-char：上一个单词只有一个字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_features(tokens, i):\n",
    "    return {\n",
    "        'next-word-capitalized': tokens[i + 1][0].isupper(),\n",
    "        'prevword': tokens[i - 1].lower(),\n",
    "        'punct': tokens[i],\n",
    "        'prev-word-is-one-char': len(tokens[i - 1]) == 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于这一特征提取器，我们可以通过选择所有的标点符号创建一个加标签的特征及链表，然后标注它们是否是边界标识符，并利用特征集训练和评估一个标点符号分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'next-word-capitalized': False, 'prevword': 'nov', 'punct': '.', 'prev-word-is-one-char': False}, False)\n",
      "0.936026936026936\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries)) \n",
    "               for i in range(1, len(tokens) - 1) \n",
    "               if tokens[i] in '.?!']\n",
    "print(featuresets[0])\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这种分类器进行断句，我们只需要检查每个标点符号，看它是否是作为一个边界标识符，在边界标识符处分割词链表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
    "            sents.append(words[start: i+ 1])\n",
    "            start = i + 1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents\n",
    "\n",
    "words = tokens[:1000]\n",
    "sents = segment_sentences(words)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 识别对话行为类型\n",
    "识别对话中言语下的对话行为，如问候、陈述、情感、说明等，是理解谈话重要的第一步。这里我们使用 NPS 聊天语料库，包含 10000 个来自即时消息会话的帖子以及对应的行为类型标签，建立一个分类器来识别新的即时消息帖子的对话行为类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'contains(now)': True, 'contains(im)': True, 'contains(left)': True, 'contains(with)': True, 'contains(this)': True, 'contains(gay)': True, 'contains(name)': True}, 'Statement')]\n",
      "0.668\n"
     ]
    }
   ],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains(%s)' % word.lower()] = True\n",
    "    return features\n",
    "\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
    "print(featuresets[:1])\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 识别文字蕴含\n",
    "识别文字蕴含是判断文本 T 的一个给定片段是否蕴含着另一个叫做“假设 H”的文本，示例如下：\n",
    "\n",
    "> **Challenge 3, Pair 34 (True)**  \n",
    "**T:** Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation\n",
    "Organisation (SCO), the fledgling association that binds Russia, China and four\n",
    "former Soviet republics of central Asia together to fight terrorism.  \n",
    "**H:** China is a member of SCO.\n",
    "\n",
    "我们可以把识别文字蕴含当作一个分类任务，尝试为每一对预测真/假标签。比较理想的假设是如果有一个蕴含，那么假设所表示的所有信息也应该在文本中表示。相反，如果假设中有的资料文本中没有，那么就没有蕴含。  \n",
    "nltk 内置的 RTEFeatureExtractor 类可以帮助我们分析除去部分停用词后的文本和假设中的词汇，并计算它们之间的重叠和差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Co', 'terrorism.', 'at', 'fight', 'Iran', 'that', 'meeting', 'Shanghai', 'Organisation', 'association', 'was', 'together', 'Russia', 'Asia', 'four', 'Soviet', 'China', 'fledgling', 'republics', 'Davudi', 'central', 'representing', 'operation', 'Parviz', 'SCO', 'binds', 'former'}\n",
      "{'member', 'China', 'SCO.'}\n",
      "set()\n",
      "{'China'}\n",
      "{'member'}\n"
     ]
    }
   ],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "print(extractor.text_words)\n",
    "print(extractor.hyp_words)\n",
    "print(extractor.overlap('word'))  # Same words between text and hypothesis\n",
    "print(extractor.overlap('ne'))  # All-cap word or title-cased word in overlap('word')\n",
    "print(extractor.hyp_extra('word'))  # Extraneous material in the hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用文本和假设间的重叠和差异作为特征，我们可以训练和评估如下文字蕴含识别器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'word_overlap': 2, 'word_hyp_extra': 0, 'ne_overlap': 1, 'ne_hyp_extra': 1}, 1)]\n",
      "0.4625\n"
     ]
    }
   ],
   "source": [
    "def rte_features(rtepair):  # RTE stands for Recognizing textual entailment.\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features\n",
    "\n",
    "rtepairs = nltk.corpus.rte.pairs(['rte3_dev.xml'])\n",
    "featuresets = [(rte_features(rtepair), rtepair.value) for rtepair in rtepairs]\n",
    "print(featuresets[:1])\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

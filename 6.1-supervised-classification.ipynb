{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有监督分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性别鉴定\n",
    "男性和女性的名字有一些鲜明的特点，以 a, e 和 i 结尾的很可能是女性，而以 k, o, r, s 结尾的很可能是男性，让我们建立一个分类器来更精确地模拟这些差异。\n",
    "一开始，我们只寻找给定名称的最后一个字母作为特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'k'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gender_features(name):\n",
    "    return {\n",
    "        'last_letter': name[-1]\n",
    "    }\n",
    "gender_features('Shrek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用特征提取器处理名称数据，并划分特征集的结果链表为一个训练集和一个测试集，训练集用于训练一个新的朴素贝叶斯分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo -> male\n",
      "Trinity -> female\n",
      "Accuracy: 0.750000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "        [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(names)\n",
    "featuresets = [(gender_features(name), gender) for (name, gender) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Neo -> %s' % classifier.classify(gender_features('Neo')))\n",
    "print('Trinity -> %s' % classifier.classify(gender_features('Trinity')))\n",
    "print('Accuracy: %f' % nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以检查分类器，确定哪些特征对于区分名字的性别是最有效的，可以看出以 a 结尾的名字中女性是男性的 387倍，而以 k 结尾的名字中男性是女性的 30 倍，这些比率成为似然比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     34.1 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.1 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.5 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在处理大型语料库时，构建一个包含每一个实例特征的链表会使用大量的内存，在这些情况下使用函数 [nltk.classify.apply_features](https://www.nltk.org/_modules/nltk/classify/util.html#apply_features)，返回一个行为像一个链表而不会在内存存储所有特征集的对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import apply_features\n",
    "train_set = apply_features(gender_features, names[500:])\n",
    "test_set = apply_features(gender_features, names[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择正确的特征\n",
    "用于一个给定的学习算法的特征数目是有限的，如果你提供太多的特征，那么该算法将高度依赖训练数据的特性，而一般化到新的例子的效果不会太好。这个问题被称为**过拟合**，当运作在小训练集上尤其会有问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770000\n"
     ]
    }
   ],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features['firstletter'] = name[0].lower()\n",
    "    features['lastletter'] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count(%s)' % letter] = name.lower().count(letter)\n",
    "        features['has(%s)' % letter] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "featuresets = [(gender_features2(name), gender) for (name, gender) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: %f' % nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦初始特征集被选定，完善特征集的一个非常有效的方法是**错误分析**。首先我们选择一个**开发集**，包含用户创建模型的语料数据，然后将这种开发集分为**训练集**和**开发测试集**。训练集用于训练模型，开发测试集用于进行错误分析，测试集用于系统的最终评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.804000\n"
     ]
    }
   ],
   "source": [
    "train_names, devtest_names, test_names = names[1500:], names[500:1500], names[:500]\n",
    "train_set = [(gender_features2(name), gender) for (name, gender) in train_names]\n",
    "devtest_set = [(gender_features2(name), gender) for (name, gender) in devtest_names]\n",
    "test_set = [(gender_features2(name), gender) for (name, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: %f' % nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用开发测试集，我们可以生成一个分类器预测名字性别时的错误列表，检查个别错误按理，尝试确定什么额外信息将使其能够作出正确的决定，然后相应的调整特征集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=male     guess=female   name=Abbey                         \n",
      "correct=male     guess=female   name=Abbie                         \n",
      "correct=male     guess=female   name=Adair                         \n",
      "correct=male     guess=female   name=Adams                         \n",
      "correct=male     guess=female   name=Agustin                       \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))\n",
    "for (tag, guess, name) in sorted(errors)[:5]:\n",
    "    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "浏览错误列表，它明确指出一些多个字母的后缀可以提示名字性别。例如 yn 结尾的名字显示以女性为主，尽管 n 结尾的名字往往是男性；以 ch 结尾的名字通常是男性，尽管以 h 结尾的名字倾向于是女性。因此我们调整特征提取器，包含两个字母后缀的特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.804000\n"
     ]
    }
   ],
   "source": [
    "def gender_features(name):\n",
    "    return {\n",
    "        'suffix1': name[-1:],\n",
    "        'suffix2': name[-2:]\n",
    "    }\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: %f' % nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不断重复这个错误分析过程，检查存在于由新改进的分类器产生的错误中的模式，每一次错误分析过程被重复，我们应该选择一个不同的开发测试/训练分割，以确保该分类器不会开始反映开发测试集的特质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档分类\n",
    "使用文档已经按类别标记的语料库，我们可以建立分类器，自动给新文档添加适当的类别标签。这里我们选择电影评论语料库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们为文档定义一个特征提取器。对于文档主题识别，我们可以为每个词定义一个特性表示该文档是否包含这个词。为了限制分类器需要处理的特征的数目，我们从整个语料库中选择前 2000 个最频繁的词，然后简单地检查这些词是否在一个给定的文档中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(,)': True, 'contains(the)': True, 'contains(.)': True, 'contains(a)': True, 'contains(and)': True, 'contains(of)': True, 'contains(to)': True, \"contains(')\": True, 'contains(is)': True, 'contains(in)': True}\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = [w for (w,_) in all_words.most_common(2000)]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)  # Checking word in a set is much faster than in a list.\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % (word,)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "test_doc_features = document_features(movie_reviews.words('pos/cv957_8737.txt'))\n",
    "print(dict(list(test_doc_features.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用这些特征来训练分类器，为新的电影评论加标签，并使用 show_most_informative_features() 来找出哪些特征是分类器发现最有信息量的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.810000\n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     10.7 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      8.3 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.2 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      6.4 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      6.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: %f' % nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第5章中，我们建立了一个正则表达式标注器，通过查找词内部的组成，为词选择词性标记。然而，这个正则表达式标注器是手工制作的。作为替代，我们可以训练一个分类器来算出哪个后缀最有信息量。首先，让我们找出最常见的后缀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "common_suffixes = [w for (w, _) in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们定义一个特征提取函数，检查给定的单词的这些后缀，并训练一个新的决策树分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.627051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n, g) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size: ], featuresets[: size]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "print('Accuracy: %f' % nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型的一个很好的性质是它们往往很容易解释。我们甚至可以指示NLTK将它们以伪代码形式输出:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(the) == False: \n",
      "  if endswith(,) == False: \n",
      "    if endswith(s) == False: \n",
      "      if endswith(.) == False: return '.'\n",
      "      if endswith(.) == True: return '.'\n",
      "    if endswith(s) == True: \n",
      "      if endswith(is) == False: return 'PP$'\n",
      "      if endswith(is) == True: return 'BEZ'\n",
      "  if endswith(,) == True: return ','\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索上下文语境\n",
    "如果特征提取器仅仅看着目标词，我们就没法添加依赖词出现的上下文语境特征。然而上下文语境往往提供关于正确标记的强大线索，例如：标注此 fly，如果知道它前面的词是 a 将使我们能够确定它是一个名词，而不是动词。这里我们修改特征提取器来利用词的上下文特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\n",
    "        'suffix(1)': sentence[i][-1:],\n",
    "        'suffix(2)': sentence[i][-2:],\n",
    "        'suffix(3)': sentence[i][-3:]\n",
    "    }\n",
    "    if i == 0:\n",
    "        features['prev-word'] = '<START>'\n",
    "    else:\n",
    "        features['prev-word'] = sentence[i - 1]\n",
    "    return features\n",
    "\n",
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.789160\n"
     ]
    }
   ],
   "source": [
    "featuresets = []\n",
    "for tagged_sent in brown.tagged_sents(categories='news'):\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append((pos_features(untagged_sent, i), tag))\n",
    "        \n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size: ], featuresets[: size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy %f' % nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，利用上下文特征提高了我们词性标注器的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列分类\n",
    "一种序列分类器策略，成为**连续分类**或**贪婪序列分类**，是为第一个输入找到最有可能的类标签，然后使用这个问题的答案帮助找到下一个输入的最佳标签，这个过程可以不断重复直到所有的输入都被贴上标签。\n",
    "在下面的例子中，我们扩展特征提取函数使其具有参数history， 它提供一个我们到目前为止已经为句子预测的标记的链表，history中的每个标记对应句子中的一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.798053\n"
     ]
    }
   ],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\n",
    "        'suffix(1)': sentence[i][-1:],\n",
    "        'suffix(2)': sentence[i][-2:],\n",
    "        'suffix(3)': sentence[i][-3:]\n",
    "    }\n",
    "    if i == 0:\n",
    "        features['prev-word'] = '<START>'\n",
    "        features['prev-tag'] = '<START>'\n",
    "    else:\n",
    "        features['prev-word'] = sentence[i - 1]\n",
    "        features['prev-tag'] = history[i - 1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print('Accuracy %f' % tagger.evaluate(test_sents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
